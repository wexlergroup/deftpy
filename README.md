TODO:

parallelization in data_processing.py

unit testing for ^

clarification about data/features/Ehull

better data handling? data/papers/kumagai is massive (almost 1GB) (pickle? BZ2?)

^ can also look into parquet/feather as more lightweight data storage

graphing_kumagai, graphing_witman -> visualization.py (deftpy website/dashboard? (if we do this, likely using django)

BV_scratch, corr, getting_indexes should all be transitioned from scripts into functions
